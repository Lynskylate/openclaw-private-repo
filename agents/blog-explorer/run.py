#!/usr/bin/env python3
"""
Blog Explorer - æŠ“å–å’Œå­˜å‚¨ AI å…¬å¸åšå®¢å†…å®¹
"""

import os
import sys
import json
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ  workspace è·¯å¾„
workspace_path = Path("/opt/openclaw/.openclaw/workspace")
sys.path.insert(0, str(workspace_path / "skills" / "tavily"))

from tavily_scripts.tavily_search import search as tavily_search

# Blog é…ç½®
BLOGS = {
    "openai": {
        "name": "OpenAI",
        "url": "https://openai.com/blog",
        "search_query": "site:openai.com/blog",
        "wiki_space": "7606015010138590169",
        "wiki_parent": "AIå…¬å¸åšå®¢"
    },
    "anthropic": {
        "name": "Anthropic",
        "url": "https://www.anthropic.com/blog",
        "search_query": "site:anthropic.com/blog OR site:anthropic.com/news",
        "wiki_space": "7606015010138590169",
        "wiki_parent": "AIå…¬å¸åšå®¢"
    },
    "langchain": {
        "name": "LangChain",
        "url": "https://blog.langchain.com",
        "search_query": "site:blog.langchain.com",
        "wiki_space": "7606015010138590169",
        "wiki_parent": "AIå…¬å¸åšå®¢"
    },
    "manus": {
        "name": "Manus",
        "url": "https://www.manus.ai/blog",
        "search_query": "site:manus.ai/blog OR site:manus.im/blog",
        "wiki_space": "7606015010138590169",
        "wiki_parent": "AIå…¬å¸åšå®¢"
    }
}

PROXY_URL = "http://localhost:7890"


def search_recent_posts(blog_key, hours=24):
    """æœç´¢æœ€è¿‘å‘å¸ƒçš„æ–‡ç« """
    config = BLOGS[blog_key]
    
    query = f"{config['search_query']} after:{hours}h"
    
    print(f"ğŸ” æœç´¢ {config['name']} çš„æœ€æ–°æ–‡ç« ...")
    
    result = tavily_search(
        query=query,
        search_depth="basic",
        max_results=10,
        api_key=os.getenv("TAVILY_API_KEY")
    )
    
    if not result.get("success"):
        print(f"âŒ æœç´¢å¤±è´¥: {result.get('error', 'Unknown error')}")
        return []
    
    posts = []
    for item in result.get("results", []):
        posts.append({
            "title": item.get("title", ""),
            "url": item.get("url", ""),
            "snippet": item.get("content", ""),
            "published_date": item.get("publishedDate", ""),
            "score": item.get("score", 0)
        })
    
    print(f"âœ… æ‰¾åˆ° {len(posts)} ç¯‡æ–‡ç« ")
    return posts


def fetch_article_content(url, use_proxy=True):
    """è·å–æ–‡ç« çš„å®Œæ•´å†…å®¹ï¼ˆé€šè¿‡ä»£ç†ï¼‰"""
    import subprocess
    
    print(f"ğŸ“¥ æŠ“å–æ–‡ç« : {url}")
    
    # ä½¿ç”¨ web_fetch (é€šè¿‡ä»£ç†)
    proxy_arg = f"--proxy={PROXY_URL}" if use_proxy else ""
    cmd = f"web_fetch {proxy_arg} {url}"
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            content = result.stdout
            print(f"âœ… æˆåŠŸæŠ“å–ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.stderr}")
            return None
            
    except subprocess.TimeoutExpired:
        print(f"â±ï¸ æŠ“å–è¶…æ—¶")
        return None
    except Exception as e:
        print(f"âŒ æŠ“å–å¼‚å¸¸: {e}")
        return None


def parse_article_content(content, title, url):
    """è§£ææ–‡ç« å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
    lines = content.split('\n')
    
    # æå–å…³é”®ä¿¡æ¯
    metadata = {
        "title": title,
        "url": url,
        "fetched_at": datetime.now().isoformat(),
        "word_count": len(content.split()),
        "char_count": len(content)
    }
    
    # å°è¯•æå–æ‘˜è¦ï¼ˆå‰500å­—ç¬¦ï¼‰
    metadata["summary"] = content[:500] + "..." if len(content) > 500 else content
    
    return metadata


def create_feishu_document(blog_key, title, url, content, metadata):
    """åœ¨ Feishu çŸ¥è¯†åº“åˆ›å»ºæ–‡æ¡£"""
    from datetime import datetime
    
    company_name = BLOGS[blog_key]["name"]
    today = datetime.now().strftime("%Y-%m-%d")
    
    # æ ¼å¼åŒ–æ–‡æ¡£æ ‡é¢˜
    doc_title = f"{today}-{title[:50]}"  # é™åˆ¶æ ‡é¢˜é•¿åº¦
    folder_name = company_name
    
    # æ„å»º Markdown å†…å®¹
    markdown_content = f"""# {title}

**æ¥æº**: {url}  
**å‘å¸ƒæ—¶é—´**: {metadata.get('published_date', 'Unknown')}  
**æŠ“å–æ—¶é—´**: {metadata.get('fetched_at', '')}  
**å­—æ•°**: {metadata.get('word_count', 0)} è¯  

---

{content}

---

*æœ¬æ–‡ç”± Blog Explorer Agent è‡ªåŠ¨æŠ“å–å’Œå­˜å‚¨*
"""
    
    return markdown_content, doc_title, folder_name


def generate_daily_summary(all_posts):
    """ç”Ÿæˆæ¯æ—¥ç®€æŠ¥"""
    today = datetime.now().strftime("%Y-%m-%d")
    
    summary = f"""# AI å…¬å¸åšå®¢æ¯æ—¥ç®€æŠ¥ - {today}

## ğŸ“Š ä»Šæ—¥ç»Ÿè®¡

"""
    
    # æŒ‰å…¬å¸åˆ†ç»„
    by_company = {}
    for post in all_posts:
        company = post.get("company", "Unknown")
        if company not in by_company:
            by_company[company] = []
        by_company[company].append(post)
    
    # ç»Ÿè®¡
    for company, posts in by_company.items():
        summary += f"\n### {company}\n"
        summary += f"- æ–°æ–‡ç« æ•°: **{len(posts)}** ç¯‡\n"
        
        for post in posts[:5]:  # åªåˆ—å‡ºå‰5ç¯‡
            title = post.get("title", "Untitled")
            url = post.get("url", "")
            summary += f"  - [{title}]({url})\n"
    
    summary += f"\n## ğŸ”— æ‰€æœ‰æ–‡ç« é“¾æ¥\n\n"
    for post in all_posts:
        summary += f"- [{post.get('title', '')}]({post.get('url', '')}) - {post.get('company', '')}\n"
    
    return summary


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¦ Blog Explorer Agent å¯åŠ¨")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().isoformat()}")
    
    # 1. æœç´¢æ‰€æœ‰åšå®¢çš„æœ€æ–°æ–‡ç« 
    all_recent_posts = []
    
    for blog_key in BLOGS.keys():
        print(f"\n{'='*60}")
        print(f"å¤„ç† {BLOGS[blog_key]['name']} åšå®¢")
        print(f"{'='*60}")
        
        posts = search_recent_posts(blog_key, hours=24)
        
        # æ·»åŠ å…¬å¸ä¿¡æ¯
        for post in posts:
            post["company"] = BLOGS[blog_key]["name"]
            post["blog_key"] = blog_key
        
        all_recent_posts.extend(posts)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ€»è®¡æ‰¾åˆ° {len(all_recent_posts)} ç¯‡æ–°æ–‡ç« ")
    print(f"{'='*60}\n")
    
    if not all_recent_posts:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡ç« ")
        return
    
    # 2. åªå¤„ç†ä»Šå¤©çš„æ–°æ–‡ç« ï¼ˆé¿å…é‡å¤ï¼‰
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå…¨éƒ¨å¤„ç†
    articles_to_process = all_recent_posts[:10]  # é™åˆ¶å¤„ç†æ•°é‡
    
    print(f"ğŸ“ å°†å¤„ç† {len(articles_to_process)} ç¯‡æ–‡ç« \n")
    
    # 3. æŠ“å–å’Œå­˜å‚¨æ–‡ç« 
    processed_articles = []
    
    for i, post in enumerate(articles_to_process, 1):
        print(f"\n[{i}/{len(articles_to_process)}] {post['title']}")
        print(f"URL: {post['url']}")
        print(f"å‘å¸ƒæ—¶é—´: {post.get('published_date', 'Unknown')}")
        
        # æŠ“å–å†…å®¹
        content = fetch_article_content(post['url'], use_proxy=True)
        
        if content:
            metadata = parse_article_content(content, post['title'], post['url'])
            
            article_info = {
                "title": post['title'],
                "url": post['url'],
                "company": post['company'],
                "blog_key": post['blog_key'],
                "metadata": metadata,
                "content": content
            }
            
            processed_articles.append(article_info)
            print(f"âœ… æ–‡ç« å¤„ç†å®Œæˆ\n")
        else:
            print(f"â­ï¸ è·³è¿‡æ­¤æ–‡ç« \n")
    
    # 4. ç”Ÿæˆç®€æŠ¥
    print(f"\n{'='*60}")
    print("ğŸ“‹ ç”Ÿæˆæ¯æ—¥ç®€æŠ¥")
    print(f"{'='*60}\n")
    
    summary = generate_daily_summary([{
        "title": a["title"],
        "url": a["url"],
        "company": a["company"]
    } for a in processed_articles])
    
    # ä¿å­˜ç®€æŠ¥åˆ°æœ¬åœ°
    summary_path = workspace_path / "blog-daily-summary.md"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    print(f"âœ… ç®€æŠ¥å·²ä¿å­˜: {summary_path}")
    print(f"\nâ° å®Œæˆæ—¶é—´: {datetime.now().isoformat()}")
    print("ğŸ¦ Blog Explorer Agent å®Œæˆ")


if __name__ == "__main__":
    main()
